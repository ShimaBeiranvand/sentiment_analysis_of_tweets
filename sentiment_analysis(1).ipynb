{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk import download\n",
        "\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "download(['twitter_samples'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzwQnPOaUOxD",
        "outputId": "cc8d3456-abba-4637-fa20-eab4026cb9dc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "id": "vzwQnPOaUOxD"
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# putting all tweets together\n",
        "all_tweets = all_positive_tweets + all_negative_tweets"
      ],
      "metadata": {
        "id": "f8PiyxaWUOxM"
      },
      "execution_count": 41,
      "outputs": [],
      "id": "f8PiyxaWUOxM"
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_positive_tweets))\n",
        "print(len(all_negative_tweets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk-mCg1amkFz",
        "outputId": "7c0a1c77-c80c-4e6b-f91f-333ec6c1b0fd"
      },
      "id": "vk-mCg1amkFz",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so classes are balanced"
      ],
      "metadata": {
        "id": "4JElyZOCmspl"
      },
      "id": "4JElyZOCmspl",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating y (labels)\n",
        "\n",
        "# assigning 1 to positive tweets\n",
        "# assigning 0 to nagative tweets\n",
        "y = [ 1 for _ in range(len(all_positive_tweets)) ] + [ 0 for _ in range(len(all_negative_tweets)) ]\n",
        "\n"
      ],
      "metadata": {
        "id": "YceuPXarUOxQ"
      },
      "execution_count": 44,
      "outputs": [],
      "id": "YceuPXarUOxQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# creating X (texts)"
      ],
      "metadata": {
        "id": "hmz4HvRuUOxU"
      },
      "execution_count": 45,
      "outputs": [],
      "id": "hmz4HvRuUOxU"
    },
    {
      "cell_type": "code",
      "source": [
        "# step1 : preprocessing the tweets"
      ],
      "metadata": {
        "id": "Iaa6_JYbaYGk"
      },
      "id": "Iaa6_JYbaYGk",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2VjeoU2q0SA",
        "outputId": "2529c107-abae-4002-8d35-9d8610609649"
      },
      "id": "I2VjeoU2q0SA",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# expand contractions (don't -> do not)\n",
        "\n",
        "import contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    expanded_text = contractions.fix(text)\n",
        "    return expanded_text\n"
      ],
      "metadata": {
        "id": "a14lIHSwqrjR"
      },
      "id": "a14lIHSwqrjR",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function replaces online slang abbreviations that are used on social media with their original expansion\n",
        "\n",
        "def replace_abbreviations(tweet, slang_dict):\n",
        "    # Split the tweet into words\n",
        "    words = tweet.split()\n",
        "\n",
        "    # Replace abbreviations with their meanings\n",
        "    replaced_tweet = [\n",
        "        slang_dict[word] if word in slang_dict else word\n",
        "        for word in words\n",
        "    ]\n",
        "\n",
        "    # Join the words back into a single string\n",
        "    return ' '.join(replaced_tweet)\n",
        "\n"
      ],
      "metadata": {
        "id": "Hvp91zWApOPN"
      },
      "id": "Hvp91zWApOPN",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "slang_dict = {\n",
        "    \"lol\": \"laugh out loud\",\n",
        "    \"brb\": \"be right back\",\n",
        "    \"gtg\": \"got to go\",\n",
        "    \"ttyl\": \"talk to you later\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"idk\": \"i don’t know\",\n",
        "    \"bff\": \"best friends forever\",\n",
        "    \"fyi\": \"for your information\",\n",
        "    \"tmi\": \"too much information\",\n",
        "    \"smh\": \"shaking my head\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"imho\": \"in my humble opinion\",\n",
        "    \"lmao\": \"laughing my ass off\",\n",
        "    \"yolo\": \"you only live once\",\n",
        "    \"fomo\": \"fear of missing out\",\n",
        "    \"dm\": \"direct message\",\n",
        "    \"nsfw\": \"not safe for work\",\n",
        "    \"tldr\": \"too long; didn’t read\",\n",
        "    \"bae\": \"before anyone else\",\n",
        "    \"wtf\": \"what the f***\",\n",
        "    \"ppl\": \"people\",\n",
        "    \"bffl\": \"best friends for life\",\n",
        "    \"jk\": \"just kidding\",\n",
        "    \"tbt\": \"throwback thursday\",\n",
        "    \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"wbu\": \"what about you?\",\n",
        "    \"hbu\": \"how about you?\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"icymi\": \"in case you missed it\",\n",
        "    \"bffae\": \"best friends forever and ever\",\n",
        "    \"dms\": \"direct messages\",\n",
        "    \"sfw\": \"safe for work\",\n",
        "    \"fomo\": \"fear of missing out\",\n",
        "    \"yoyo\": \"you're on your own\",\n",
        "    \"l8r\": \"later\",\n",
        "    \"cya\": \"see you\",\n",
        "    \"xoxo\": \"hugs and kisses\",\n",
        "    \"wyd\": \"what you doing?\",\n",
        "    \"nbd\": \"no big deal\",\n",
        "    \"btt\": \"back to topic\",\n",
        "    \"b4\": \"before\",\n",
        "    \"tldr\": \"too long; didn't read\",\n",
        "    \"fml\": \"f*** my life\",\n",
        "    \"iirc\": \"if i recall correctly\",\n",
        "    \"dm\": \"direct message\",\n",
        "    \"smdh\": \"shaking my damn head\",\n",
        "    \"bruh\": \"bro (used to express disbelief)\",\n",
        "    \"sus\": \"suspicious\",\n",
        "    \"vibes\": \"good feelings or atmosphere\",\n",
        "    \"k\": \"okay\",\n",
        "    \"bae\": \"before anyone else\",\n",
        "    \"h8\": \"hate\",\n",
        "    \"tbf\": \"to be fair\",\n",
        "    \"qotd\": \"quote of the day\",\n",
        "    \"ootd\": \"outfit of the day\",\n",
        "    \"rip\": \"rest in peace\",\n",
        "    \"bop\": \"a good song\",\n",
        "    \"fomo\": \"fear of missing out\",\n",
        "    \"simp\": \"someone who shows excessive sympathy\",\n",
        "    \"lit\": \"exciting or excellent\",\n",
        "    \"goat\": \"greatest of all time\",\n",
        "    \"binge\": \"to consume excessively\",\n",
        "    \"clt\": \"can't live without\",\n",
        "    \"dnd\": \"do not disturb\",\n",
        "    \"litaf\": \"lit as f***\",\n",
        "    \"nvm\": \"never mind\",\n",
        "    \"so\": \"shout out\",\n",
        "    \"bff\": \"best friend forever\",\n",
        "    \"tmi\": \"too much information\",\n",
        "    \"lqtm\": \"laughing quietly to myself\",\n",
        "    \"fyp\": \"for you page (tiktok)\",\n",
        "    \"wya\": \"where you at?\",\n",
        "    \"slay\": \"to succeed or look great\",\n",
        "    \"hml\": \"hit me up\",\n",
        "    \"wis\": \"what i said\",\n",
        "    \"pov\": \"point of view\",\n",
        "    \"rip\": \"rest in peace\",\n",
        "    \"ngl\": \"not gonna lie\",\n",
        "    \"ymmv\": \"your mileage may vary\",\n",
        "    \"cba\": \"can’t be arsed\",\n",
        "    \"hth\": \"hope this helps\",\n",
        "    \"tldr\": \"too long; didn't read\",\n",
        "    \"pita\": \"pain in the a**\",\n",
        "    \"vaf\": \"very annoying friend\",\n",
        "    \"wth\": \"what the heck\",\n",
        "    \"kms\": \"kill myself (used humorously)\",\n",
        "    \"mfw\": \"my face when\",\n",
        "    \"fubar\": \"f***ed up beyond all recognition\",\n",
        "    \"snafu\": \"situation normal: all f***ed up\",\n",
        "    \"tba\": \"to be announced\",\n",
        "    \"eta\": \"estimated time of arrival\",\n",
        "    \"hbd\": \"happy birthday\",\n",
        "    \"lqtm\": \"laughing quietly to myself\",\n",
        "    \"rsvp\": \"répondez s'il vous plaît (please respond)\",\n",
        "    \"grwm\": \"get ready with me\",\n",
        "    \"sis\": \"sister (used as a term of endearment)\",\n",
        "    \"fam\": \"family or close friends\",\n",
        "    \"g2g\": \"got to go\",\n",
        "    \"wbu\": \"what about you?\",\n",
        "    \"fomo\": \"fear of missing out\",\n",
        "    \"goat\": \"greatest of all time\",\n",
        "    \"cya\": \"see you\",\n",
        "    \"so\": \"shout out\",\n",
        "    \"fomo\": \"fear of missing out\",\n",
        "    \"hbu\": \"how about you?\",\n",
        "    \"tldr\": \"too long; didn't read\",\n",
        "    \"simp\": \"someone who shows excessive sympathy\",\n",
        "    \"lfg\": \"looking for group (gaming)\",\n",
        "    \"nbd\": \"no big deal\",\n",
        "    \"bff\": \"best friends forever\",\n",
        "    \"ppl\": \"people\",\n",
        "    \"lfg\": \"looking for group\",\n",
        "    \"sis\": \"sister (used informally)\",\n",
        "    \"rip\": \"rest in peace\",\n",
        "    \"fomo\": \"fear of missing out\",\n",
        "    \"wya\": \"where you at?\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"ftw\": \"for the win\",\n",
        "    \"tldr\": \"too long; didn't read\",\n",
        "    \"bda\": \"big deal alert\",\n",
        "    \"srs\": \"serious\",\n",
        "    \"fomo\": \"fear of missing out\",\n",
        "    \"mia\": \"missing in action\",\n",
        "    \"bff\": \"best friends forever\",\n",
        "    \"bffl\": \"best friends for life\",\n",
        "    \"cya\": \"see you\",\n",
        "    \"dm\": \"direct message\",\n",
        "    \"mfw\": \"my face when\",\n",
        "    \"tmi\": \"too much information\",\n",
        "    \"sop\": \"standard operating procedure\",\n",
        "    \"nbd\": \"no big deal\",\n",
        "    \"so\": \"shout out\",\n",
        "    \"p2p\": \"peer to peer\",\n",
        "    \"fomo\": \"fear of missing out\",\n",
        "    \"yolo\": \"you only live once\",\n",
        "    \"tldr\": \"too long; didn't read\",\n",
        "    \"accnt\" : \"account\",\n",
        "    \"rqst\" : \"request\",\n",
        "    \"fb\" : \"facebook\",\n",
        "    \"lwwf\" : \"love wins we fight\",\n",
        "    \":)\": \"smile\",\n",
        "    \":(\": \"frown\",\n",
        "    \":D\": \"laughing\",\n",
        "    \":P\": \"playful or teasing\",\n",
        "    \":O\": \"surprised\",\n",
        "    \":|\": \"neutral\",\n",
        "    \":S\": \"confused\",\n",
        "    \";)\": \"wink\",\n",
        "    \":/\": \"unsure or skeptical\",\n",
        "    \":*\": \"kiss\",\n",
        "    \"B)\": \"cool\",\n",
        "    \":'(\": \"crying\",\n",
        "    \"XD\": \"laughing hard\",\n",
        "    \"^-^\": \"happy\",\n",
        "    \"^_^\": \"smiling\",\n",
        "    \"o_O\": \"bewildered\",\n",
        "    \">_<\": \"frustrated\",\n",
        "    \"<3\": \"heart\",\n",
        "    \"</3\": \"broken heart\",\n",
        "    \">:(\": \"angry\",\n",
        "    \"T_T\": \"crying\",\n",
        "    \":-)\": \"smile\",\n",
        "    \":-(\": \"frown\",\n",
        "    \":-D\": \"laughing\",\n",
        "    \":-P\": \"playful or teasing\",\n",
        "    \":-O\": \"surprised\",\n",
        "    \":-|\": \"neutral\",\n",
        "    \":-S\": \"confused\",\n",
        "    \";-)\": \"wink\",\n",
        "    \":-/\": \"unsure or skeptical\",\n",
        "    \":-*\": \"kiss\",\n",
        "    \"B-)\": \"cool\",\n",
        "    \":'-(\": \"crying\",\n",
        "    \"X-D\": \"laughing hard\",\n",
        "    \"^-^\": \"happy\",\n",
        "    \"^_^\": \"smiling\",\n",
        "    \"o_O\": \"bewildered\",\n",
        "    \">_<\": \"frustrated\",\n",
        "    \"<3\": \"heart\",\n",
        "    \"</3\": \"broken heart\",\n",
        "    \">:(\" : \"angry\",\n",
        "    \"T_T\": \"crying\",\n",
        "    \"O_O\": \"surprised\",\n",
        "    \"0_o\": \"confused\",\n",
        "    \":-X\": \"sealed lips\",\n",
        "    \":3\": \"cute\",\n",
        "    \":v\": \"peace\",\n",
        "    \":-]\": \"happy\",\n",
        "    \":-[\": \"sad\",\n",
        "    \":?\": \"questioning\",\n",
        "    \":-c\": \"disappointed\",\n",
        "    \":-b\": \"playful\",\n",
        "    \"D:\": \"shocked\",\n",
        "    \"D8\": \"disgusted\",\n",
        "    \"X_x\": \"dead or tired\",\n",
        "    \":-@\": \"screaming\",\n",
        "    \"O.O\": \"shocked\",\n",
        "    \":-)\": \"happy\",\n",
        "    \":-(\": \"sad\",\n",
        "    \":-/\": \"confused\",\n",
        "    \":-|\": \"meh\",\n",
        "    \":-*\": \"kiss\",\n",
        "    \":^)\": \"smirking\",\n",
        "    \":<\": \"sad\",\n",
        "    \":-]\": \"happy\",\n",
        "    \":-<\": \"sad\",\n",
        "    \":}\": \"smirk\",\n",
        "    \":)\": \"smile\",\n",
        "    \":(\": \"frown\",\n",
        "    \":D\": \"grin\",\n",
        "    \":P\": \"teasing\",\n",
        "    \";)\": \"wink\",\n",
        "    \":|\": \"neutral\",\n",
        "    \":o\": \"surprise\",\n",
        "    \":S\": \"confusion\",\n",
        "    \":X\": \"sealed lips\",\n",
        "}"
      ],
      "metadata": {
        "id": "oSk-ZJAf39e0"
      },
      "id": "oSk-ZJAf39e0",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing\n",
        "\n",
        "def normalizer(text):\n",
        "\n",
        "    # # Convert to lowercase\n",
        "    # normalized_text = text.lower()\n",
        "\n",
        "    # # Remove links, tags, and hashtags using regular expressions\n",
        "    # normalized_text = re.sub(r\"http\\S+|www\\S+|https\\S+|@\\w+|#\\w+\", \"\", normalized_text)\n",
        "\n",
        "    # Remove special characters and punctuations\n",
        "    normalized_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    normalized_text = re.sub(r'\\s+', ' ', normalized_text).strip()\n",
        "\n",
        "    return normalized_text\n",
        "\n"
      ],
      "metadata": {
        "id": "sCz3kSbwUOxW"
      },
      "execution_count": 52,
      "outputs": [],
      "id": "sCz3kSbwUOxW"
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the normalizer function\n",
        "\n",
        "test_text = normalizer(all_tweets[0])\n",
        "print('original: ' , all_tweets[0])\n",
        "print('after normalizing: ',test_text)"
      ],
      "metadata": {
        "id": "VvH-H5hE29fz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1c96dd-8f60-4086-e3b5-24d4165e590b"
      },
      "id": "VvH-H5hE29fz",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original:  #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "after normalizing:  FollowFriday FranceInte PKuchly MilipolParis for being top engaged members in my community this week\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading stop words (the words that carry little meaningful information for specific tasks)\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P02i2CW7ZTnV",
        "outputId": "1ac159d3-dab8-4b65-e2ad-0ae3b735209c"
      },
      "id": "P02i2CW7ZTnV",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# customizing stop words\n",
        "default_stopwords = set(stopwords.words('english'))\n",
        "customized_stopwords = default_stopwords - set(['no', 'not', 'ain','aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\",'weren',\"weren't\",'won',\"won't\",'wouldn',\"wouldn't\"])\n"
      ],
      "metadata": {
        "id": "vPfCOp4cUOxb"
      },
      "execution_count": 55,
      "outputs": [],
      "id": "vPfCOp4cUOxb"
    },
    {
      "cell_type": "code",
      "source": [
        "#The Punkt tokenizer is a pre-trained unsupervised machine learning model used for sentence splitting and word tokenization.\n",
        "nltk.download('punkt_tab')\n",
        "# The Averaged Perceptron Tagger is a pre-trained model used for part-of-speech (POS) tagging.\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J96ux5xNaLVw",
        "outputId": "dccaba7b-2d99-4bfe-8514-09cfd9f36d7f"
      },
      "id": "J96ux5xNaLVw",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a general function containing all steps of preprocessing\n",
        "\n",
        "def preprocessor(all_tweets):\n",
        "\n",
        "    processed_tweets = []\n",
        "\n",
        "    for tweet in all_tweets:\n",
        "\n",
        "        # Convert to lowercase\n",
        "        tweet = tweet.lower()\n",
        "        #print('lower: ')\n",
        "        #print(tweet)\n",
        "        #print('------------------------------------------------')\n",
        "\n",
        "        # Remove links, tags, and hashtags using regular expressions\n",
        "        tweet = re.sub(r\"http\\S+|www\\S+|https\\S+|@\\w+|#\\w+\", \"\", tweet)\n",
        "        #print('remove tags:')\n",
        "        #print(tweet)\n",
        "        #print('------------------------------------------------')\n",
        "\n",
        "        # expanding contractions\n",
        "        tweet = expand_contractions(tweet)\n",
        "        #print('expanded : ')\n",
        "        #print(tweet)\n",
        "        #print('------------------------------------------------')\n",
        "\n",
        "        # expanding online abbreviations\n",
        "        tweet = replace_abbreviations(tweet , slang_dict)\n",
        "        #print('abbr: ')\n",
        "        #print(tweet)\n",
        "        #print('------------------------------------------------')\n",
        "\n",
        "        # normalizing\n",
        "        normalized_tweet = normalizer(tweet)\n",
        "        #print('normalized:')\n",
        "        #print(normalized_tweet)\n",
        "        #print('------------------------------------------------')\n",
        "\n",
        "        # tokenize into words\n",
        "        tokenized_tweet = word_tokenize(normalized_tweet)\n",
        "        #print('tokenize')\n",
        "        #print(tokenized_tweet)\n",
        "        #print('------------------------------------------------')\n",
        "\n",
        "        # deleting stop words\n",
        "        filtered_tokens = [token for token in tokenized_tweet if token not in customized_stopwords]\n",
        "        #print('removing stop words: ')\n",
        "        #print(filtered_tokens)\n",
        "        #print('------------------------------------------------')\n",
        "\n",
        "        # pos tagging\n",
        "        tagged_tokens = nltk.pos_tag(filtered_tokens)\n",
        "        #print('pos tagging: ')\n",
        "        #print(tagged_tokens)\n",
        "        #print('------------------------------------------------')\n",
        "\n",
        "        # finding roots of words\n",
        "        stemmer = PorterStemmer()\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        root_tokens = []\n",
        "        for token in tagged_tokens:\n",
        "            if token[1] == 'JJ':\n",
        "                root = stemmer.stem(token[0])\n",
        "            else:\n",
        "                root = lemmatizer.lemmatize(token[0])\n",
        "\n",
        "            root_tokens.append(root)\n",
        "\n",
        "        #print('root tokens:')\n",
        "        #print(root_tokens)\n",
        "        #print('------------------------------------------------')\n",
        "        # joining tokens together\n",
        "        joined_tweet = ' '.join(root_tokens)\n",
        "        #print('joined tokens: ')\n",
        "        #print(joined_tweet)\n",
        "        #print('------------------------------------------------')\n",
        "        processed_tweets.append(joined_tweet)\n",
        "\n",
        "    return processed_tweets\n"
      ],
      "metadata": {
        "id": "UivvGKSXUOxe"
      },
      "execution_count": 57,
      "outputs": [],
      "id": "UivvGKSXUOxe"
    },
    {
      "cell_type": "code",
      "source": [
        "processed_tweets = preprocessor(all_tweets)"
      ],
      "metadata": {
        "id": "f7VF_1RTdfzg"
      },
      "id": "f7VF_1RTdfzg",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(10):\n",
        "  print('original tweet:')\n",
        "  print(all_tweets[idx])\n",
        "  print('processed tweet:')\n",
        "  print(processed_tweets[idx])\n",
        "  print('----------------------------------------------------')"
      ],
      "metadata": {
        "id": "A1bNhyT2UOxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7affeda-95a3-4026-a2eb-df92da16e8c2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original tweet:\n",
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "processed tweet:\n",
            "top engaged member community week smile\n",
            "----------------------------------------------------\n",
            "original tweet:\n",
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "processed tweet:\n",
            "hey james odd unsur skeptic please call contact centre abl assist smile mani thanks\n",
            "----------------------------------------------------\n",
            "original tweet:\n",
            "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
            "processed tweet:\n",
            "listen last night smile bleed amazing track scotland\n",
            "----------------------------------------------------\n",
            "original tweet:\n",
            "@97sides CONGRATS :)\n",
            "processed tweet:\n",
            "congrats smile\n",
            "----------------------------------------------------\n",
            "original tweet:\n",
            "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
            "processed tweet:\n",
            "yeaaaah yippppi account verified request succeed got blue tick mark facebook profile smile day\n",
            "----------------------------------------------------\n",
            "original tweet:\n",
            "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
            "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
            "processed tweet:\n",
            "one irresist smile\n",
            "----------------------------------------------------\n",
            "original tweet:\n",
            "We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\n",
            "processed tweet:\n",
            "not like keep love customer waiting long hope enjoy happi friday love win fight smile\n",
            "----------------------------------------------------\n",
            "original tweet:\n",
            "@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.\n",
            "processed tweet:\n",
            "second thought not enough time dd smile new short entering system sheep must buying\n",
            "----------------------------------------------------\n",
            "original tweet:\n",
            "Jgh , but we have to go to Bayan :D bye\n",
            "processed tweet:\n",
            "jgh go bayan bye\n",
            "----------------------------------------------------\n",
            "original tweet:\n",
            "As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\n",
            "\n",
            "Well… as the name implies :p.\n",
            "processed tweet:\n",
            "act mischievousness calling etl layer inhouse warehousing app katamari well name implies p\n",
            "----------------------------------------------------\n"
          ]
        }
      ],
      "id": "A1bNhyT2UOxg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step2: convert the text data to numerical type (TF_IDF)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(processed_tweets)"
      ],
      "metadata": {
        "id": "1Z5VPp6tUOxk"
      },
      "execution_count": 60,
      "outputs": [],
      "id": "1Z5VPp6tUOxk"
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(X.shape)\n",
        "print(type(X))"
      ],
      "metadata": {
        "id": "DURUJHx1UOxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b114da-58cc-4061-e043-31c4508576bc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 9135)\t0.42847524637642237\n",
            "  (0, 2741)\t0.5279066400733814\n",
            "  (0, 5589)\t0.4593422200086518\n",
            "  (0, 1761)\t0.4359033601146696\n",
            "  (0, 9762)\t0.3425431208483619\n",
            "  (0, 8178)\t0.13921472094152296\n",
            "  (1, 8178)\t0.0822377318060899\n",
            "  (1, 3959)\t0.2142193622412416\n",
            "  (1, 4523)\t0.296297291132831\n",
            "  (1, 6272)\t0.3384323535186648\n",
            "  (1, 9438)\t0.3032898673574324\n",
            "  (1, 8117)\t0.2932274142380037\n",
            "  (1, 6843)\t0.1707298000103171\n",
            "  (1, 1323)\t0.2408036267690537\n",
            "  (1, 1838)\t0.2932274142380037\n",
            "  (1, 1444)\t0.3384323535186648\n",
            "  (1, 30)\t0.25867955993522845\n",
            "  (1, 539)\t0.36501661804647695\n",
            "  (1, 5428)\t0.23521514362735854\n",
            "  (1, 8920)\t0.15465827475283603\n",
            "  (2, 8178)\t0.10991697654672804\n",
            "  (2, 5137)\t0.3649523008293712\n",
            "  (2, 4981)\t0.2792872928413901\n",
            "  (2, 6106)\t0.2819858990849044\n",
            "  (2, 1015)\t0.46708761924648545\n",
            "  :\t:\n",
            "  (9994, 3300)\t0.12768143181595187\n",
            "  (9994, 9653)\t0.6088159899324703\n",
            "  (9995, 1476)\t0.4399763088825327\n",
            "  (9995, 9698)\t0.2778233627185309\n",
            "  (9995, 597)\t0.5589352743960707\n",
            "  (9995, 3300)\t0.1325173103868571\n",
            "  (9995, 9488)\t0.6318746301549797\n",
            "  (9996, 3209)\t0.5470932508040123\n",
            "  (9996, 3300)\t0.13977321742531607\n",
            "  (9996, 7139)\t0.6179331659168098\n",
            "  (9996, 1188)\t0.5470932508040123\n",
            "  (9997, 650)\t0.4584112949315774\n",
            "  (9997, 6760)\t0.517131616339042\n",
            "  (9997, 4514)\t0.7227959435841655\n",
            "  (9998, 1873)\t0.4930282572154627\n",
            "  (9998, 5849)\t0.42303309364366704\n",
            "  (9998, 3300)\t0.11152038698288436\n",
            "  (9998, 192)\t0.5317562141416589\n",
            "  (9998, 5491)\t0.5317562141416589\n",
            "  (9999, 9762)\t0.2661467227306654\n",
            "  (9999, 7642)\t0.1988215474963976\n",
            "  (9999, 2892)\t0.4451350420445569\n",
            "  (9999, 4148)\t0.4801009298660906\n",
            "  (9999, 8651)\t0.4801009298660906\n",
            "  (9999, 5719)\t0.4801009298660906\n",
            "(10000, 10228)\n",
            "<class 'scipy.sparse._csr.csr_matrix'>\n"
          ]
        }
      ],
      "id": "DURUJHx1UOxp"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "ar8x8eKVUOxr"
      },
      "execution_count": 62,
      "outputs": [],
      "id": "ar8x8eKVUOxr"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Train the logistic regression model\n",
        "# model = LogisticRegression()\n",
        "# model.fit(X_train, y_train)\n",
        "# # predicting the test set\n",
        "# y_pred = model.predict(X_test)\n",
        "\n",
        "# # Evaluating the model\n",
        "# print(f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "s_HddxNra4VY"
      },
      "id": "s_HddxNra4VY",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the model\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# Define the parameter grids for each model\n",
        "param_grid_lr = {\n",
        "    'C': [0.1, 1, 10],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],  # Regularization type\n",
        "    'solver': ['liblinear']  # Solver for optimization\n",
        "}\n",
        "\n",
        "param_grid_nb = {\n",
        "    'alpha': [0.1, 0.5, 1.0],  # Smoothing parameter\n",
        "    'fit_prior': [True, False]  # Whether to learn class prior probabilities\n",
        "}\n",
        "\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],  # Regularization parameter\n",
        "    'kernel': ['linear', 'rbf'],  # Kernel type\n",
        "    'gamma': ['scale', 'auto']  # Kernel coefficient for 'rbf'\n",
        "}\n",
        "\n",
        "# Initialize the models\n",
        "log_reg = LogisticRegression()\n",
        "naive_bayes = MultinomialNB()\n",
        "svm = SVC()\n",
        "\n",
        "# Perform Grid Search for Logistic Regression\n",
        "grid_search_lr = GridSearchCV(log_reg, param_grid_lr, cv=5, scoring='f1', verbose=1)\n",
        "grid_search_lr.fit(X_train, y_train)\n",
        "\n",
        "# Perform Grid Search for Naive Bayes\n",
        "grid_search_nb = GridSearchCV(naive_bayes, param_grid_nb, cv=5, scoring='f1', verbose=1)\n",
        "grid_search_nb.fit(X_train, y_train)\n",
        "\n",
        "# Perform Grid Search for SVM\n",
        "grid_search_svm = GridSearchCV(svm, param_grid_svm, cv=5, scoring='f1', verbose=1)\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "\n",
        "# Get the best models\n",
        "best_lr = grid_search_lr.best_estimator_\n",
        "best_nb = grid_search_nb.best_estimator_\n",
        "best_svm = grid_search_svm.best_estimator_\n",
        "\n",
        "# Evaluate the best models on the test set\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "print(\"Logistic Regression Results:\")\n",
        "evaluate_model(best_lr, X_test, y_test)\n",
        "\n",
        "print(\"Naive Bayes Results:\")\n",
        "evaluate_model(best_nb, X_test, y_test)\n",
        "\n",
        "print(\"SVM Results:\")\n",
        "evaluate_model(best_svm, X_test, y_test)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Logistic Regression Parameters:\", grid_search_lr.best_params_)\n",
        "print(\"Best Naive Bayes Parameters:\", grid_search_nb.best_params_)\n",
        "print(\"Best SVM Parameters:\", grid_search_svm.best_params_)"
      ],
      "metadata": {
        "id": "NJdcKEUpUOxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd59ebe-f032-41ee-80c2-8959920875d8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Logistic Regression Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.92      0.94       508\n",
            "           1       0.92      0.96      0.94       492\n",
            "\n",
            "    accuracy                           0.94      1000\n",
            "   macro avg       0.94      0.94      0.94      1000\n",
            "weighted avg       0.94      0.94      0.94      1000\n",
            "\n",
            "F1 Score: 0.9414\n",
            "Naive Bayes Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.91       508\n",
            "           1       0.92      0.87      0.90       492\n",
            "\n",
            "    accuracy                           0.90      1000\n",
            "   macro avg       0.90      0.90      0.90      1000\n",
            "weighted avg       0.90      0.90      0.90      1000\n",
            "\n",
            "F1 Score: 0.8966\n",
            "SVM Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94       508\n",
            "           1       0.93      0.95      0.94       492\n",
            "\n",
            "    accuracy                           0.94      1000\n",
            "   macro avg       0.94      0.94      0.94      1000\n",
            "weighted avg       0.94      0.94      0.94      1000\n",
            "\n",
            "F1 Score: 0.9408\n",
            "Best Logistic Regression Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Best Naive Bayes Parameters: {'alpha': 1.0, 'fit_prior': True}\n",
            "Best SVM Parameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "id": "NJdcKEUpUOxx"
    }
  ]
}